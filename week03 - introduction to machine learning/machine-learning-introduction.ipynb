{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Machine Learning?\n",
    "Loosely following Chapter 1 in Python Machine Learning 3rd Edition, Raschka.\n",
    "\n",
    "<img src='./diagrams/a-machine-learning.jpg'>\n",
    "\n",
    "[Image source](https://www.google.com/url?sa=i&url=https%3A%2F%2Fwww.informatec.com%2Fen%2Fmachine-learning&psig=AOvVaw1id7LiQnbAWJSImoPrFrnN&ust=1630778280221000&source=images&cd=vfe&ved=0CAsQjRxqFwoTCKDcxeKw4_ICFQAAAAAdAAAAABAD)\n",
    "\n",
    "#### Definition 1:\n",
    ">Vast amounts of data are being generated in many fields, and the statistician’s job is to make sense of it all: to extract import patterns and trends, and understand “what the data says.” We call this learning from data.\n",
    "<br><br>Hastie et al., The Elements of Statistical Learning.\n",
    "\n",
    "#### Definion 2:\n",
    "> Machine learning is a subfield of computer science that is concerned with building algorithms which, to be useful, rely on a collection of examples of some phenomenon. These examples can come from nature, be handcrafted by humans or generated by another algorithm.\n",
    "<br><br>Burkov, The Hundred-Page Machine Learning Book\n",
    "\n",
    "#### Definition 3:\n",
    "> Machine learning (ML) is the study of computer algorithms that can improve automatically through experience and by the use of data. It is seen as a part of artificial intelligence. Machine learning algorithms build a model based on sample data, known as \"training data\", in order to make predictions or decisions without being explicitly programmed to do so. Machine learning algorithms are used in a wide variety of applications, such as in medicine, email filtering, speech recognition, and computer vision, where it is difficult or unfeasible to develop conventional algorithms to perform the needed tasks.\n",
    "<br><br>A subset of machine learning is closely related to computational statistics, which focuses on making predictions using computers; but not all machine learning is statistical learning. The study of mathematical optimization delivers methods, theory and application domains to the field of machine learning. Data mining is a related field of study, focusing on exploratory data analysis through unsupervised learning. Some implementations of machine learning use data and neural networks in a way that mimics the working of a biological brain. In its application across business problems, machine learning is also referred to as predictive analytics.  \n",
    "<br>Source: https://en.wikipedia.org/wiki/Machine_learning\n",
    "\n",
    "#### Definition 4:\n",
    ">A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E. (Tom Mitchell)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning vs. Artificial Intelligence\n",
    "<img src='./diagrams/ai-ml.jpeg'>\n",
    "\n",
    "[Image source](https://www.google.com/url?sa=i&url=https%3A%2F%2Fwww.bbntimes.com%2Fscience%2Fartificial-intelligence-vs-machine-learning-vs-artificial-neural-networks-vs-deep-learning&psig=AOvVaw2rw9Ou9dU_we-lUl0PjbBt&ust=1630849936510000&source=images&cd=vfe&ved=0CAsQjRxqFwoTCIDRhdC75fICFQAAAAAdAAAAABAO)\n",
    "\n",
    "\n",
    "# Machine Learning Timeline\n",
    "<img src='./diagrams/nvidia-ai-ml.png'>\n",
    "\n",
    "[Image source](https://blogs.nvidia.com/blog/2016/07/29/whats-difference-artificial-intelligence-machine-learning-deep-learning-ai/)  \n",
    "\n",
    "---\n",
    "\n",
    "Neural networks and deep learning are really accelerated in usage and popularity due to increases in data available AND exponential increases in computing. Deep learning has significantly benefited from [Graphics Processing Units (GPUs)](https://developer.nvidia.com/deep-learning).\n",
    "\n",
    "---\n",
    "\n",
    "[Detailed timeline](https://en.wikipedia.org/wiki/Timeline_of_machine_learning)<bk>\n",
    "\n",
    "#### Events of note:  \n",
    "- [1959 - Arthur Samual popularizes the term \"machine learning\" and teaches a computer to play checkers](https://en.wikipedia.org/wiki/Arthur_Samuel)  \n",
    "- [1970s - AI Winter: project shutdowns, general lack of advancement]  (https://en.wikipedia.org/wiki/AI_winter)  \n",
    "- [1986 - Hilton's backpropagation](https://en.wikipedia.org/wiki/Backpropagation)  \n",
    "- 1990s - Support vector machines and neural networks gain traction  \n",
    "- [2009 - ImageNet](https://en.wikipedia.org/wiki/ImageNet)  \n",
    "- 2010s - Deep learning becomes popular  \n",
    "- [2010 - Kaggle](https://www.kaggle.com)  \n",
    "- [2011 - Watson Wins Jeopardy](https://en.wikipedia.org/wiki/Watson_(computer))  \n",
    "- 2011 - 2014 Siri, Cortana, Alexa\n",
    "- [2016 - AlphaGo](https://en.wikipedia.org/wiki/AlphaGo)  \n",
    "- [2020 - AI to detect misinformation](https://ai.facebook.com/blog/heres-how-were-using-ai-to-help-detect-misinformation/)  \n",
    "- [2020 - GPT3 Language Generation Model](https://en.wikipedia.org/wiki/GPT-3)\n",
    "- [2022 - ChatGPT](https://en.wikipedia.org/wiki/ChatGPT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional Examples\n",
    "[Google Search](https://www.google.com/?client=safari)\n",
    "<br>[Apple Face ID](https://support.apple.com/en-us/HT208109)\n",
    "<br>[Cancer Detection](https://www.nature.com/articles/d41586-020-00847-2)\n",
    "<br>[Fake News Detection](https://arxiv.org/pdf/1805.08751.pdf)\n",
    "<br>[Retirement Planners](https://www.aiplanner.com)\n",
    "<br>[Creating New Flavors](https://www.ibm.com/blogs/research/2019/02/ai-new-flavor-experiences/)\n",
    "<br>[Fraud Detection](https://aws.amazon.com/solutions/implementations/fraud-detection-using-machine-learning/)\n",
    "<br>[Computer Vision](https://en.wikipedia.org/wiki/Computer_vision)\n",
    "<br>[Natural Language Processing](https://en.wikipedia.org/wiki/Natural_language_processing)\n",
    "<br>[Spam Filtering](https://en.wikipedia.org/wiki/Naive_Bayes_spam_filtering)\n",
    "<br>[DeepFakes](https://www.theguardian.com/technology/2020/jan/13/what-are-deepfakes-and-how-can-you-spot-them)\n",
    "\n",
    "# Resources\n",
    "[Neural Networks vs. Deep Learning](https://www.ibm.com/cloud/blog/ai-vs-machine-learning-vs-deep-learning-vs-neural-networks)\n",
    "<br>[Raschka's GitHub](https://github.com/rasbt/python-machine-learning-book-3rd-edition)\n",
    "<br>[Raschka's Website](https://sebastianraschka.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Before machines could learn we used rules [rule-based models](https://en.wikipedia.org/wiki/Rule-based_modeling).\n",
    "\n",
    "```python\n",
    "dogOrCat = ''\n",
    "if whiskers and eatsMice:\n",
    "    dogOrCat = 'Cat'\n",
    "elif name in ['Garfield', 'Felix the Cat']:\n",
    "    dogOrCat = 'Cat'\n",
    "elif sleepsAllDay:\n",
    "    dogOrCat = 'Cat'\n",
    "else:\n",
    "    dogOrCat = 'Dog'\n",
    "```\n",
    "\n",
    "\n",
    "# Explicit Programming vs. Machine Learning\n",
    "<img src='./diagrams/programming-vs-learning.png'>\n",
    "\n",
    "[Image source](https://github.com/rasbt/stat479-machine-learning-fs19/blob/master/01_overview/01-ml-overview__notes.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old School: Explicitly Defining Spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/msaricaumbc/DS_data/master/ds602/week3/spam.csv', encoding='latin-1')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.iloc[:, :2]\n",
    "df.columns = ['label', 'message']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label'].value_counts().plot.barh()\n",
    "plt.title('Distribution of Class Labels', loc='left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = df['label'].value_counts()\n",
    "counts / counts.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__A subject matter expert (SME) reviewed messages and identified a key word list. The programmer takes the list and implements the first spam detector.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of words that indicate the message contains spam - provided by a SME\n",
    "spamWords = set(['free', 'text', 'winner', 'win', 'urgent', \n",
    "             'txt', 'charged', 'sms', 'prize', 'account', \n",
    "             'laid', 'freemsg', 'partner','bonus', 'congrats'\n",
    "                ])\n",
    "\n",
    "# split message into tokens\n",
    "def split_words(x):\n",
    "    x = x.lower()\n",
    "    x = x.translate(str.maketrans('', '', string.punctuation))\n",
    "    x = x.split(' ')\n",
    "    return x\n",
    "\n",
    "# run tokens through the spam list\n",
    "def eval_spam(x, spamList = spamWords):\n",
    "    is_spam = 'ham'\n",
    "    for word in x:\n",
    "        if word in spamList:\n",
    "            is_spam = 'spam'\n",
    "            break\n",
    "    return is_spam\n",
    "    \n",
    "df['words'] = df['message'].apply(lambda x: split_words(x))\n",
    "df['explicit'] = df['words'].apply(lambda x: eval_spam(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(df['label'] == df['explicit']) / len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['dummy'] = 'ham'\n",
    "sum(df['label'] == df['dummy']) / len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__How does it perform? [Confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix)__\n",
    "\n",
    "<img src='./diagrams/cm.png' style=\"width: 400px;\">\n",
    "\n",
    "[Image source](https://www.google.com/url?sa=i&url=https%3A%2F%2Ftowardsdatascience.com%2Fconfusion-matrix-for-your-multi-class-machine-learning-model-ff9aa3bf7826&psig=AOvVaw1BGzxd0qgbSOLBDGjHBsll&ust=1631231267101000&source=images&cd=vfe&ved=0CAsQjRxqFwoTCODV_ZfI8PICFQAAAAAdAAAAABAD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./diagrams/fnfp.webp' style=\"width: 400px;\">\n",
    "\n",
    "[Image source](https://neeraj-kumar-vaid.medium.com/statistical-performance-measures-12bad66694b7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explicitCM = df.pivot_table(index='explicit', columns='label', values='message', aggfunc='count')\n",
    "explicitCM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics we can use to evaluate the performance:\n",
    "\n",
    "#### Accuracy:\n",
    "$$\\frac{TP + TN}{TP + TN + FN + FP}$$\n",
    "\n",
    "#### Recall:\n",
    "$$\\frac{TP}{TP + FN}$$\n",
    "\n",
    "#### Precision:\n",
    "$$\\frac{TP}{TP + FP}$$\n",
    "\n",
    "----\n",
    "\n",
    "<img src='./diagrams/Precisionrecall.svg.png' style=\"width: 400px;\">\n",
    "\n",
    "[Image source](https://en.wikipedia.org/wiki/Precision_and_recall)\n",
    "\n",
    "**Note**: We'll go over these in more detail later. There's many metrics and different methods for evaluating performance for classification problems.  \n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./diagrams/cm-metrics.png'>\n",
    "\n",
    "[Image source](https://en.wikipedia.org/wiki/Template:Diagnostic_testing_diagram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explicitCM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(m):\n",
    "    return (m.loc['ham', 'ham'] + m.loc['spam','spam']) / m.to_numpy().sum()\n",
    "\n",
    "eA = accuracy(explicitCM)\n",
    "\n",
    "print(f'Accuracy: {eA:.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(m):\n",
    "    return (m.loc['spam','spam'])/(m.loc['spam','spam'] + m.loc['ham','spam'])\n",
    "    \n",
    "def precision(m):\n",
    "    return (m.loc['spam','spam'])/(m.loc['spam','spam'] + m.loc['spam','ham'])\n",
    "\n",
    "eR = recall(explicitCM)\n",
    "eP = precision(explicitCM)\n",
    "\n",
    "print(f'Recall: {eR:.2%}')\n",
    "print(f'Precision: {eP:.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(f'Accuracy: {eA:.2%}')\n",
    "print(f'Recall: {eR:.2%}')\n",
    "print(f'Precision: {eP:.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pain points for this rule-based approach\n",
    "- Need to know the words ahead of time.    \n",
    "- Need to monitor and update lists.  \n",
    "- Precision likely poor.  \n",
    "- Complex relationships aren't possible.  \n",
    "\n",
    "# Machine Learning identities underlying patterns without manually coding\n",
    "#### Machines can learn from the data  \n",
    "#### Don't need to have priori on the signal drivers\n",
    "Since we have messages and their spam/ham label, we can train models to learn the patterns within the data that explain the label.\n",
    "\n",
    "Steps:\n",
    "- [Convert text to a matrix of word counts](https://en.wikipedia.org/wiki/Bag-of-words_model)  \n",
    "- [Fit a naive Bayes classifier model](https://en.wikipedia.org/wiki/Naive_Bayes_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['message']\n",
    "y = df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count words in each measure\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "counter = CountVectorizer()\n",
    "X = counter.fit_transform(X)\n",
    "\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features = counter.get_feature_names_out()\n",
    "# # features\n",
    "# values = X.todense()\n",
    "\n",
    "# pd.DataFrame(data=values, columns=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a naive bayes model\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "model = BernoulliNB().fit(X, y)\n",
    "\n",
    "# predict class from above model\n",
    "preds = model.predict(X)\n",
    "df['ml_prediction'] = preds\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does it perform?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "machineCM = df.pivot_table(index='ml_prediction', columns='label', values='message', aggfunc='count')\n",
    "machineCM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mA = accuracy(machineCM)\n",
    "mR = recall(machineCM)\n",
    "mP = precision(machineCM)\n",
    "\n",
    "print(f'Accuracy: {mA:.2%}')\n",
    "print(f'Recall: {mR:.2%}')\n",
    "print(f'Precision: {mP:.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks great!\n",
    "\n",
    "__not so fast__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.head(20).message[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 = counter.transform(['you win $10000 come and claim it'])\n",
    "model.predict(X2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Takeaways:\n",
    "- Big improvement with less effort and subject matter expertise.  \n",
    "- Clearly SPAM is more complicated than containing simple key words. Machine learning can detect those more complicated patterns.  \n",
    "- I could have no idea want words are correlated with spam, but if the dataset is labeled, the algorithms allow us to learn those patterns.  \n",
    "\n",
    ">You'll still want to make sure you understand the data first and perform rigorous exploratory data analysis. \n",
    "\n",
    ">This was a pretty lazy example. We are evaluating the performance based on the data that we trained on instead of data that was withheld from the model. We'll discuss different evaluation methods in a few weeks.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Types of Machine Learning\n",
    "We'll be focused on supervised and unsupervised learning in our course. The SPAM example above is a common example of supervised learning.\n",
    "\n",
    "<img src='./diagrams/learning-types.png'  style=\"width: 600px;\">\n",
    "\n",
    "[Image Source](https://github.com/rasbt/python-machine-learning-book-3rd-edition/blob/master/ch01/ch01.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning\n",
    "[scikit-learn supervised learning](https://scikit-learn.org/stable/supervised_learning.html#supervised-learning)\n",
    "\n",
    "## Goal is to detect a function that utilizes data (features) to explain a known phenomena\n",
    "### Requirements: label (categorical or continuous), matrix of input features that *may* explain the label\n",
    "\n",
    "$y=f(x)$\n",
    "\n",
    "- We observe an outcome (can be binary, categorical, continuous)\n",
    "- We observe features $X_{i}=(X_{i1},\\dots,X_{in})$ in the outcome $Y_{i}$\n",
    "- We can model this as $Y_{i}=f(X_{i})+\\epsilon_{i}$  \n",
    "- $\\epsilon_{i}$ is the error or noise that we won't be able to capture from the function (at least without overfitting). We want to minimize this!\n",
    "- Our goal is to estimate this unknown function ￼ that is the true data generating process\n",
    "- We may choose different methods depending on the goal\n",
    "- Prediction - care about predicting ￼ to our best ability\n",
    "- Inference - care about the interpretation of our ￼ and how the inputs (￼) relate to ￼\n",
    "\n",
    "## Interpretability or Black box  \n",
    "- Do you need to understand why a prediction is being made?  \n",
    "- Some models will offer a degree of feature importance intrepretations.  \n",
    "- Some models are basically black boxes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "Class of learning underneath supervised learning. Goal is to predict labels (usually categorical) of new unlabeled examples, based on training data that contained the correct label. The email spam example earlier is an example.\n",
    "\n",
    "- Single/binary class (spam/ham) or multiclass (dog/cat/fish).  \n",
    "- Looking to create a decision boundary with a function. In 2-dimensions it can be visually, but gets complicated in larger feature spaces.  \n",
    "\n",
    "<img src='./diagrams/supervised-classification.png'  style=\"width: 400px;\">\n",
    "\n",
    "[Image source](https://github.com/rasbt/python-machine-learning-book-3rd-edition/blob/master/ch01/ch01.ipynb)\n",
    "\n",
    "### Example with the [Titantic Dataset](https://www.kaggle.com/c/titanic) using Logistic Regression\n",
    "\n",
    "<img src='./diagrams/titantic.jpg'   style=\"width: 400px;\">\n",
    "\n",
    "[Image source](https://en.wikipedia.org/wiki/Titanic#/media/File:RMS_Titanic_3.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "titantic = pd.read_csv('https://raw.githubusercontent.com/msaricaumbc/DS_data/master/ds602/week3/titantic.csv')\n",
    "titantic.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titantic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titantic['Survived'].value_counts().plot.barh()\n",
    "plt.title('Distribution of Passenger Survival', loc='left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into training/test\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "categorical = ['Sex', 'Pclass', 'Siblings/Spouses Aboard', 'Parents/Children Aboard']\n",
    "cols = ['Pclass', 'Sex', 'Age', 'Siblings/Spouses Aboard', 'Parents/Children Aboard','Fare']\n",
    "\n",
    "def gen_splits(dataframe, features, target, test_pct=0.2):\n",
    "    return train_test_split(dataframe[cols], dataframe[target], test_size=test_pct)\n",
    "\n",
    "X_train, X_test, y_train, y_test = gen_splits(titantic, cols, 'Survived')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pipeline with transformations\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def pipe(model):\n",
    "    pipeline = Pipeline([('t', transformer), ('m', model)])\n",
    "    return pipeline\n",
    "\n",
    "transformer = ColumnTransformer(transformers=[('ohe', OneHotEncoder(handle_unknown='ignore'), categorical)])\n",
    "\n",
    "lgr = pipe(LogisticRegression())\n",
    "lgr.fit(X_train, y_train)\n",
    "print('Model fit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "labs = {0: 'No Survive', 1:'Survived'}\n",
    "cm = confusion_matrix(y_test.map(labs), pd.Series(lgr.predict(X_test)).map(labs), labels=['No Survive','Survived'])\n",
    "ConfusionMatrixDisplay(cm, display_labels=['No Survive','Survived']).plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Training accuracy: {lgr.score(X_train, y_train):.2%}')\n",
    "print(f'Test accuracy: {lgr.score(X_test, y_test):.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Considerations\n",
    "- May need to create a customized way to evaluate models.  \n",
    "- If looking using for workload triaging, may want to consider using ranked probabilites. \n",
    "- What is the cost difference between a false-positive and a false-negative?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression\n",
    "The other major class of supervised learning. Here the goal is to predict a real-value number. This is based on training data that contains examples with the response of interest. For example, I may want to predict weight based on height. I'll have a dataset with weight and height, train a model to determine the relationship, and use that model to predict the weights of new examples where I only know the height.\n",
    "\n",
    "<img src='./diagrams/supervised-regression.png'  style=\"width: 400px;\">\n",
    "     \n",
    "[Image source](https://github.com/rasbt/python-machine-learning-book-3rd-edition/blob/master/ch01/ch01.ipynb)\n",
    "\n",
    "\n",
    "### Example with [ISRL's advertising dataset](https://www.kaggle.com/ishaanv/ISLR-Auto) Using Least-Squares Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adData = pd.read_csv('https://raw.githubusercontent.com/msaricaumbc/DS_data/master/ds602/week3/advertising.csv')\n",
    "adData.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.plotting.scatter_matrix(adData)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can hypothesize $Sales_{i}=f(TV_{i},Radio_{i},Newspaper_{i})+\\epsilon{i}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "\n",
    "results = smf.ols('Sales ~ TV + Radio + Newspaper', data=adData).fit()\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Increased newspaper budgets reduce sales? You'd probably get a question about that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictedSales = results.predict(adData)\n",
    "\n",
    "plt.plot(adData['Sales'], predictedSales, 'bo')\n",
    "plt.xlabel('Actual Sales')\n",
    "plt.ylabel('Predicted Sales')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Learning\n",
    "Discovery of hidden structures in the data\n",
    "\n",
    "We observe features $X_{i}=(X_{i1},\\dots,X_{in})$ that may explain a *latent or unobserved $Y_{i}$*.\n",
    "- We may have data on user behavior for a web application. We might be able to segment these into groups that correlate with actual demographics.  \n",
    "- We may be able to extract topic groups for streams of different documents.  \n",
    "- Sometimes we don't need to decompose into groups, but need to compress the dimensionality of the data without losing significant amounts of the underlying variance.\n",
    "\n",
    "## Reducing/simplifying the dimensionality of the data\n",
    "### For compressing the feature space and/or visualization\n",
    "Reduce the number of features to increase efficiency, reduce noise, and further densify the data.\n",
    "\n",
    "<img src='./diagrams/unsupervised-reduction.png'  style=\"width: 400px;\">\n",
    "\n",
    "[Image source](https://github.com/rasbt/python-machine-learning-book-3rd-edition/blob/master/ch01/ch01.ipynb)\n",
    "\n",
    "# [t-SNE Example](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html) with [MNIST](http://yann.lecun.com/exdb/mnist/)\n",
    "\n",
    "MNIST is a database of hand-written digits that is commonly used for benchmarking model performance.\n",
    "\n",
    "#### t-SNE is [t-distributed neighborhood embedding](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "mnist = fetch_openml('mnist_784', version=1 ,parser='auto')\n",
    "mnist.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The features are pixel intensities. Nothing we've looked at so far is going to be able to handle looking at 784 features at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist.data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Target distribution (i.e., labels of the digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist.target.value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist.target.value_counts().sort_index().plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample to decrease the computational needs and runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = 2500\n",
    "\n",
    "mnist_sample = mnist.data.sample(samples)\n",
    "mnist_sample_targets = mnist.target.iloc[mnist_sample.index.tolist()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit the transform\n",
    "> This could take a while on larger dataset and/or if your machine doesn't have great specifications. Test it out on smaller datasets to get a sense of runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import datetime\n",
    "\n",
    "ts_start = datetime.datetime.now()\n",
    "tsne = TSNE(n_components=2).fit_transform(mnist_sample)\n",
    "ts_end = datetime.datetime.now()\n",
    "\n",
    "print(f'Completed in {ts_end-ts_start}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add transform and label into a DataFrame for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_df = pd.DataFrame(tsne)\n",
    "tsne_df.index = mnist_sample.index.tolist()\n",
    "tsne_df.columns = ['component1', 'component2']\n",
    "\n",
    "tsne_df = pd.concat([tsne_df, mnist_sample_targets], axis=1)\n",
    "tsne_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.set(rc={'figure.figsize':(9,9)})\n",
    "sns.scatterplot(x='component1', y='component2', hue='class', data=tsne_df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Considerations:\n",
    "- Condensing from 784 dimensions (mostly sparse) to 2 dimensions.  \n",
    "- Not uncommon for distortions to occur and clusters to materialize that aren't really related.  \n",
    "- This could direct you to potential issues in the data, e.g., what's with the handful of zeros that look like sixes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Principal Components](https://en.wikipedia.org/wiki/Principal_component_analysis)\n",
    "\n",
    "<img src='./diagrams/pca.png'  style=\"width: 400px;\">\n",
    "\n",
    "[Image source](https://en.wikipedia.org/wiki/Principal_component_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "adX = adData.iloc[:, :3]\n",
    "adXpca = PCA(n_components=2).fit(adX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adXpca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Can help reduce the feature space and remove noise that may cause unstability in modeling\n",
    ">Won't guarantee better results, but yet another knob you can try to tune."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "model = LinearRegression().fit(adXpca.transform(adX),adData['Sales'])\n",
    "pcaPredictions = model.predict(adXpca.transform(adX))\n",
    "\n",
    "plt.plot(pcaPredictions, adData['Sales'], 'bo')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "Clustering is one of the major categories of unsupervised learning. The goal is to group the data based on its feature set. We don't know or have class labels, but hope we can split out latent (hidden) groups. [scikit-learn clustering](https://scikit-learn.org/stable/modules/clustering.html#clustering)\n",
    "\n",
    "#### [k-Means](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html)\n",
    "<img src='./diagrams/unsupervised-clustering.png'  style=\"width: 400px;\">\n",
    "\n",
    "[Image source](https://github.com/rasbt/python-machine-learning-book-3rd-edition/blob/master/ch01/ch01.ipynb)\n",
    "\n",
    "[scikit-learn Demo using the Digits data](https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_digits.html#sphx-glr-auto-examples-cluster-plot-kmeans-digits-py)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-Means with iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "iris.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=3, n_init='auto').fit(iris.data)\n",
    "kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeansPredict = kmeans.predict(iris.data)\n",
    "kmeansPredict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "irisClass = pd.Series(iris.target)\n",
    "irisCluster = pd.Series(kmeansPredict)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(irisClass, irisCluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__That's not too shabby!__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Hierarchial](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html#sklearn.cluster.AgglomerativeClustering)\n",
    "<img src='./diagrams/unsupervised-hierarchy.png'  style=\"width: 400px;\">\n",
    "\n",
    "[Image source](https://en.wikipedia.org/wiki/Hierarchical_clustering)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "model = AgglomerativeClustering(distance_threshold=0, n_clusters=None)\n",
    "model = model.fit(iris.data)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = np.zeros(model.children_.shape[0])\n",
    "linkage_matrix = np.column_stack([model.children_, model.distances_, counts]).astype(float)\n",
    "\n",
    "dendrogram(linkage_matrix, no_labels=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning (outside scope of course, just for awareness)\n",
    "Goal is to get an agent (e.g., dog) to perform tasks (e.g., fetch a stick) and get rewards (e.g., treat) or penalty (e.g., no treat) for completing the task. Usually going to be a series of actions that comprise an optimized reward signal, which can be immediate or delayed. Think of it as intelligent trial and error. Chess is another popular example. [AlphaGo is another interesting example.](https://deepmind.com/research/case-studies/alphago-the-story-so-far)\n",
    "\n",
    "<img src='./diagrams/dog.png'>\n",
    "\n",
    "[Image source](https://www.mathworks.com/discovery/reinforcement-learning.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# You're a chef. The kitchen is fully stocked.\n",
    "### (You need to know how to cook though)\n",
    "\n",
    "#### Your job:\n",
    "- Determine the type of problem you are trying to solve.  \n",
    "- Evaluate the data you have been provided.  \n",
    "- Determine how to structure an experiment to test the hypothesis.  \n",
    "- Determine how to evaluate the results of your experiment.  \n",
    "\n",
    "<img src='./diagrams/ml_map.png'  style=\"width: 700px;\"><bk>\n",
    "    \n",
    "https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters vs. Hyperparameters\n",
    "\n",
    "- Many models are [parametric](https://en.wikipedia.org/wiki/Parametric_model), which means the algorithms learns weights (e.g., parameters) during the optimization process. Think of the coefficients in regression models.  \n",
    "- Many models have [hyperparameters](https://en.wikipedia.org/wiki/Hyperparameter_(machine_learning)) which are user-defined values that control parts of the learning process.   \n",
    "- Finding the best set of hyperparameters can be time consuming.  \n",
    "- Think of hyperparameters as knobs you need to tune, like on an old radio.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Data Notation\n",
    "\n",
    "## As you would see it in a pandas DataFrame or numpy array:\n",
    "<img src='./diagrams/data-and-labels.png' style=\"width: 600px;\">\n",
    "\n",
    "[Image source](https://github.com/rasbt/python-machine-learning-book-3rd-edition/blob/master/ch01/ch01.ipynb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## As you would see it expressed mathematically in matrix notation\n",
    "Most machine learning is going to be based on linear algebra and calculus. If its been awhile since you've seen either of those types of problems, it may be good to quick refresher.\n",
    "\n",
    "#### General form for the feature matrix (*m* samples and *n* features)\n",
    "$$\\begin{equation*}\n",
    "A_{m,n} = \n",
    "\\begin{pmatrix}\n",
    "a_{1,1} & a_{1,2} & \\cdots & a_{1,n} \\\\\n",
    "a_{2,1} & a_{2,2} & \\cdots & a_{2,n} \\\\\n",
    "\\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n",
    "a_{m,1} & a_{m,2} & \\cdots & a_{m,n} \n",
    "\\end{pmatrix}\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "#### iris (150 samples, 4 features)\n",
    "$$\\begin{equation*}\n",
    "iris_{150,4} = \n",
    "\\begin{pmatrix}\n",
    "a_{1,1} & a_{1,2} & \\cdots & a_{1,4} \\\\\n",
    "a_{2,1} & a_{2,2} & \\cdots & a_{2,4} \\\\\n",
    "\\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n",
    "a_{150,1} & a_{150,2} & \\cdots & a_{150,4} \n",
    "\\end{pmatrix}\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "#### Target variable general\n",
    "$$\\begin{equation*}\n",
    "y_{m} = \n",
    "\\begin{pmatrix}\n",
    "y_{1} \\\\\n",
    "y_{2} \\\\\n",
    "\\vdots  \\\\\n",
    "y_{m} \n",
    "\\end{pmatrix}\n",
    "(y\\in [class_{1}, class_{2}, ..., class_{z}])\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "#### Target variable general for iris (150 samples)\n",
    "$$\\begin{equation*}\n",
    "species_{150} = \n",
    "\\begin{pmatrix}\n",
    "species_{1} \\\\\n",
    "species_{2} \\\\\n",
    "\\vdots  \\\\\n",
    "species_{150} \n",
    "\\end{pmatrix}\n",
    "(y\\in [setosa, versicolor, virginica])\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Typical Pipeline Flow\n",
    "The algorithms/models are just a piece in the machine learning puzzle. There are many other component around organzing your data, defining your experiment, transforming your data into features, and evaluating your results.\n",
    "\n",
    "### Sample Supervised Workflow\n",
    "<img src='./diagrams/ml-pipeline2.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best Practices  \n",
    "- Preprocessing.  \n",
    "- Determine how to design your experiment.  \n",
    "- Training and selecting models.  \n",
    "- Evaluating models.  \n",
    "- Reproducibility and communicating results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No Free Lunch\n",
    "Popularized by David Wolpert. See [*The Lack of A Prior Distinctions Between Learning Algorithms, D.H. Wolpert, 1996](https://ieeexplore.ieee.org/document/6795940)\n",
    "\n",
    ">I suppose it is tempting, if the only tool you have is a hammer, to treat everything like a nail.\n",
    "<br><br>Abraham Maslow, 1966\n",
    "\n",
    "### No single model will perform the best across all problems. \n",
    "### You should compare at least a couple of different models when you are trying to solve a problem.\n",
    "\n",
    "# Must Bring Balance to the Force \n",
    "<img src='./diagrams/bias-error.png'  style=\"width: 700px;\">\n",
    "\n",
    "[Image Source - By Bigbossfarin - Own work, CC0](https://commons.wikimedia.org/w/index.php?curid=105307219)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithms We'll Explore in Future Classes\n",
    "\n",
    "[Linear Regression](https://en.wikipedia.org/wiki/Linear_regression)\n",
    "<br>[Logistic Regression](https://en.wikipedia.org/wiki/Logistic_regression)\n",
    "<br>[Support Vector Machines](https://en.wikipedia.org/wiki/Support-vector_machine)\n",
    "<br>[k-Nearest Neighbors](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm)\n",
    "<br>[Random Forest](https://en.wikipedia.org/wiki/Random_forest)\n",
    "<br>[Decision Trees](https://en.wikipedia.org/wiki/Decision_tree_learning)\n",
    "<br>[Naive Bayes](https://en.wikipedia.org/wiki/Naive_Bayes_classifier)\n",
    "<br>[Neural Networks](https://en.wikipedia.org/wiki/Neural_network)\n",
    "<br>[Principal Component Analysis](https://en.wikipedia.org/wiki/Principal_component_analysis)\n",
    "<br>[Linear Discrimant Analysis](https://en.wikipedia.org/wiki/Linear_discriminant_analysis)\n",
    "<br>[Agglomerative Clusterig](https://en.wikipedia.org/wiki/Cluster_analysis#agglomerative_clustering)\n",
    "<br>[Density-based Clustering](https://en.wikipedia.org/wiki/DBSCAN)\n",
    "<br>[k-Means Clustering](https://en.wikipedia.org/wiki/K-means_clustering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Readings\n",
    "\n",
    "[Raschka's Introduction to Machine Learning Notes](https://github.com/rasbt/stat479-machine-learning-fs19/blob/master/01_overview/01-ml-overview__notes.pdf)<br>\n",
    "[Hundred-Page Machine Learning Book: Introduction to Machine Learning](https://www.dropbox.com/s/lrhtt1wkffnm4fe/Chapter1.pdf?dl=0)<br>\n",
    "[Hundred-Page Machine Learning Book: Notation](https://www.dropbox.com/s/0cprdghmnzpck8h/Chapter2.pdf?dl=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources\n",
    "[Getting started with scikit-learn](https://scikit-learn.org/stable/getting_started.html)\n",
    "<br>[Glossary](https://scikit-learn.org/stable/glossary.html)\n",
    "<br>[Don't fall into the pit!](https://scikit-learn.org/stable/common_pitfalls.html)\n",
    "<br>[Basic tutorial](https://scikit-learn.org/stable/tutorial/basic/tutorial.html)\n",
    "<br>[Supervised learning](https://scikit-learn.org/stable/tutorial/statistical_inference/supervised_learning.html)\n",
    "<br>[Unsupervised learning](https://scikit-learn.org/stable/tutorial/statistical_inference/unsupervised_learning.html)\n",
    "<br>[Putting it together](https://scikit-learn.org/stable/tutorial/statistical_inference/putting_together.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "263.594px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
