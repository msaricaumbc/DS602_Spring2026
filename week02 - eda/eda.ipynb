{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "\n",
    "> It is important to understand what you CAN DO before you learn to measure how WELL you seem to have DONE it.  \n",
    "\n",
    "> Examples, NOT case histories\n",
    "\n",
    "> The greatest value of a picture is when it forces us to notice what we never expected to see.\n",
    "\n",
    "> To learn about data analysis, it is right that each of us try many things that do not work-that we tackle more problems than we make expert analyses of.\n",
    "\n",
    "John W. Tukey, Exploratory Data Analysis (1977)\n",
    "\n",
    "\n",
    "\n",
    "<img src='./diagrams/eda-tukey.jpg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In statistics, exploratory data analysis is an approach of analyzing data sets to summarize their main characteristics, often using statistical graphics and other data visualization methods. A statistical model can be used or not, but primarily EDA is for seeing what the data can tell us beyond the formal modeling or hypothesis testing task. Exploratory data analysis was promoted by John Tukey to encourage statisticians to explore the data, and possibly formulate hypotheses that could lead to new data collection and experiments. EDA is different from initial data analysis (IDA),[1] which focuses more narrowly on checking assumptions required for model fitting and hypothesis testing, and handling missing values and making transformations of variables as needed. EDA encompasses IDA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./diagrams/legos.jpeg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis is...\n",
    "- Detective work  \n",
    "- Will inform what is feasible  \n",
    "- Cornerstone of most analytics projects  \n",
    "- Might result in the toughest challenges you face in analytics  \n",
    "- Will likely take the bulk of your time  \n",
    "- Likely need to continually revisit during a project  \n",
    "- Messy in the real world"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis Could Include...\n",
    "- Consolidating/aggregating/merging data  \n",
    "- Extracting from non-tabular datasets\n",
    "- Understanding distributions, shape, and characteristics of your data  \n",
    "- Casting data to its proper type (e.g., string to date)  \n",
    "- Determining where there are gaps (so you can get more data)  \n",
    "- Intermediate data objects to support specific analysis  \n",
    "- Schemas to address missing data and/or outliers  \n",
    "- Lots and lots and lots of plots  \n",
    "- Creativity  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# And You'll Probably Find...\n",
    "- Numbers stored as strings, strings as numbers, …  \n",
    "- Lack of metadata on ordinal data  \n",
    "- Aggregations required  \n",
    "- Reconstructing what history looked like at time of events (record updates)  \n",
    "- Significant changes in business processes that create structural shifts  \n",
    "- Data is scattered across hundreds of Excel files  \n",
    "- You need to engineer supplemental data  \n",
    "- NULL means different things at different times. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goals\n",
    "\n",
    "The objectives of EDA are to:\n",
    "\n",
    "- Suggest hypotheses about the causes of observed phenomena  \n",
    "- Assess assumptions on which statistical inference will be based  \n",
    "- Support the selection of appropriate statistical tools and techniques  \n",
    "- Provide a basis for further data collection through surveys or experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A minute on Tidy Data\n",
    "http://vita.had.co.nz/papers/tidy-data.pdf\n",
    "\n",
    "> A huge amount of effort is spent cleaning data to get it ready for analysis, but there has been little research on how to make data cleaning as easy and effective as possible. This paper tackles a small, but important, component of data cleaning: data tidying. **Tidy datasets are easy to manipulate, model and visualise, and have a specific structure: each variable is a column, each observation is a row, and each type of observational unit is a table.** This framework makes it easy to tidy messy datasets because only a small set of tools are needed to deal with a wide range of un-tidy datasets. This structure also makes it easier to develop tidy tools for data analysis, tools that both input and output tidy datasets. The advantages of a consistent data structure and matching tools are demonstrated with a case study free from mundane data manipulation chores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./diagrams/tidy-data.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: Tidying \n",
    "Data: Football Salaries  \n",
    "Accessed Source: https://github.com/rfordatascience/tidytuesday/tree/master/data/2018/2018-04-09  \n",
    "Original source: https://www.spotrac.com/rankings/\n",
    "\n",
    "This example is to highlight some ways you may want to explore numeric data, e.g., looking at the distributions, getting summary statistics, and looking at how the data varies by dimensional attributes. \n",
    "\n",
    "Salaries are in dollars and each observation is going to be an individual player's salary.\n",
    "\n",
    "If you are new to American Football:\n",
    "\n",
    "<img src='./diagrams/football-positions.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "nfl = pd.read_excel('https://github.com/msaricaumbc/DS_data/blob/master/ds602/nfl_salary.xlsx?raw=true')\n",
    "print(f'Rows: {nfl.shape[0]:,} | Columns: {nfl.shape[1]:,}')\n",
    "nfl.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This violates the tidy principles (similar to the Census data and the WHO data).\n",
    "Let's melt it, so each observation is a row (an individual player's salary for a given year)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nflLong = nfl.melt(id_vars='year', var_name='position', value_name='salary')\n",
    "nflLong.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nflLong.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Drop the NULL records. Looks like some positions didn't have an equal number of observations entered for each year__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nflLong = nflLong.dropna()\n",
    "nflLong.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__We have: two dimensions: time (year), position, and a numerical value.__\n",
    "For numerical data, the standard summary is the 5-number summary:\n",
    "- Minimum.  \n",
    "- 1st quartile. \n",
    "- Median. \n",
    "- 3rd quartile.  \n",
    "- Maximum.  \n",
    "\n",
    "And the standard moments:  \n",
    "- Mean.  \n",
    "- Variance or standard deviation.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# /1000 to scale in ths.\n",
    "(nflLong['salary'].describe()/1000).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nflLong.groupby('year')['salary'].count().sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summaries with pivot tables\n",
    "\n",
    ">A pivot table is a table of grouped values that aggregates the individual items of a more extensive table (such as from a database, spreadsheet, or business intelligence program) within one or more discrete categories. This summary might include sums, averages, or other statistics, which the pivot table groups together using a chosen aggregation function applied to the grouped values.\n",
    "<br><br>Pivot tables are a technique in data processing. They arrange and rearrange (or \"pivot\") statistics in order to draw attention to useful information. This leads to finding figures and facts quickly making them integral to data analysis. This ultimately leads to helping businesses or individuals make educated decisions.\n",
    "<br><br>Although pivot table is a generic term, Microsoft trademarked PivotTable in the United States in 1994 (canceled in 2020)\n",
    "[Wikipedia](https://en.wikipedia.org/wiki/Pivot_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meanNflPivot = (\n",
    "    (nflLong.pivot_table(index='position', columns='year', values='salary', aggfunc='median')/1000000)\n",
    "    .round(1)\n",
    ")\n",
    "\n",
    "meanNflPivot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding a splash of color with heat maps\n",
    "[Heat maps](https://en.wikipedia.org/wiki/Heat_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.heatmap(meanNflPivot)\n",
    "plt.xlabel('')\n",
    "plt.ylabel('')\n",
    "plt.title('Median Salary by Position and Year')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trends in Salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yrAvg = nflLong.groupby('year')['salary'].mean()\n",
    "yrErr = nflLong.groupby('year')['salary'].std()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "yrAvg.plot.bar(yerr=yrErr, ax=ax, capsize=4, rot=0)\n",
    "plt.title('Mean Salary Per Year (w/ Error Bars)')\n",
    "plt.xlabel('')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Looks like it is increasing over time, that would be important to know.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nflLong['salary'].hist(bins=100)\n",
    "plt.title('Salary Distribution\\nAll years pooled', loc='right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log Transforms will compress the variance\n",
    "Also useful if performing regression, log-transforms will help with some techical issues regarding correlations to residuals, and will generally model out better. For money ($$) data, the coefficients will be elasticities, which are nice for presenting to business users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.log(nflLong['salary']).hist(bins=100)\n",
    "plt.title('Salary Distribution\\nAll years pooled', loc='right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nflLong['salaryBin'] = (nflLong['salary'] / 1000000).astype(int)\n",
    "nflLong.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nflSalaryPivot = nflLong.pivot_table(index='salaryBin', columns='year', values='salary', aggfunc='count')\n",
    "nflSalaryPivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nflSalaryPivot.plot(alpha=0.5)\n",
    "plt.legend(title='')\n",
    "plt.xlabel('Salary ($millions)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Boxplots for looking at the variance between groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nflLong.boxplot(column='salary', by='position', vert=False, showfliers=True)\n",
    "nflLong.boxplot(column='salary', by='position', vert=False, showfliers=False)\n",
    "plt.grid(False)\n",
    "plt.title('')\n",
    "plt.xlabel('Salary')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__What can we say about the salaries by position?__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numerical Comparisons\n",
    "Data: iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/msaricaumbc/DS_data/master/ds602/iris.csv')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data summaries by groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('species').describe().T\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Picture is worth 1,000 words (or 96 cells)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subs = df.boxplot(by='species')\n",
    "\n",
    "fig = subs[0][0].get_figure()\n",
    "fig.suptitle('Grouped by Species')\n",
    "\n",
    "rotation = 45\n",
    "for s in subs:\n",
    "    x1, x2 = s\n",
    "    \n",
    "    for m in x1.get_xticklabels():\n",
    "        m.set_rotation(rotation)\n",
    "        \n",
    "    for m in x2.get_xticklabels():\n",
    "        m.set_rotation(rotation)\n",
    "        \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scatterplots and Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.plotting.scatter_matrix(df, figsize=(6,6))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Scatterplot matrices are a valuable tool, but lose their utility as the number of features grow.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallel Coordinates\n",
    "Sometimes scatter plot matrices get too crowded and it's hard to see co-movement between different features.\n",
    "\n",
    "From Wikipedia:\n",
    "> To show a set of points in an n-dimensional space, a backdrop is drawn consisting of n parallel lines, typically vertical and equally spaced. A point in n-dimensional space is represented as a polyline with vertices on the parallel axes; the position of the vertex on the i-th axis corresponds to the i-th coordinate of the point.   \n",
    "This visualization is closely related to time series visualization, except that it is applied to data where the axes do not correspond to points in time, and therefore do not have a natural order. Therefore, different axis arrangements may be of interest.\n",
    "\n",
    "**Warning: Make sure your data is on the same scale, or this won't be as useful.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.plotting.parallel_coordinates(df, class_column='species')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Andrews Curve\n",
    "From Wikipedia:\n",
    "\n",
    "> In data visualization, an Andrews plot or Andrews curve is a way to visualize structure in high-dimensional data. It is basically a rolled-down, non-integer version of the Kent–Kiviat radar m chart, or a smoothed version of a parallel coordinate plot. It is named after the statistician David F. Andrews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.plotting.andrews_curves(df, 'species')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reducing the number of dimensions with Principal Component Analysis (PCA)\n",
    "[Principal Component Analysis](https://en.wikipedia.org/wiki/Principal_component_analysis)\n",
    "\n",
    "An unsupervised technique to reduce the feature space into **N** orthogonal vectors that eventually capture all the variation in the feature space. Typically used for reducing dimensionality and/or visualization.\n",
    "\n",
    "It is a handy trick for reducing the number of features from **N** to **2** in order to plot on a scatterplot.\n",
    "- If the 2 components explain a large amount of the variance this can be very helpful.  \n",
    "- If the 2 components don't explain a significant amount of the variance, it can be misleading.  \n",
    "- PCA is very scale sensitive, so be careful.  \n",
    "- We'll discuss this is more detail later.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a quick reminder of the data\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.relplot(x='sepal_length', y='sepal_width', hue='species', data=df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[:, :4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "dfPca = pd.DataFrame(pca.fit_transform(df.iloc[:, :4]), columns=['pc1','pc2'])\n",
    "dfPca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfM = df.join(dfPca)\n",
    "dfM.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.relplot(x='pc1', y='pc2', hue='species', data=dfM)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joining Data\n",
    "In organization, most of the time you'll need to assemble your data from multiple sources. There may be multiple major and intermediate steps to get to the point where you have your training data.\n",
    "\n",
    "<img src='./diagrams/join-pipeline.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sometimes how to configure the join might be a little tricky, but here's a guide:\n",
    "\n",
    "<img src='./diagrams/sql-join2.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: We want to determine relationship of urban population rates to taxes.\n",
    "- File 1: urban population. \n",
    "- File 2: taxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "population = pd.read_csv('https://raw.githubusercontent.com/msaricaumbc/DS_data/master/ds602/share-of-population-urban.csv')\n",
    "taxes = pd.read_csv('https://raw.githubusercontent.com/msaricaumbc/DS_data/master/ds602/taxes-on-incomes-of-individuals-and-corporations-gdp.csv')\n",
    "\n",
    "population.info()\n",
    "print('\\n-------------------------------------\\n')\n",
    "taxes.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Left Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leftJoin = pd.merge(population, taxes, how='left', on=['Entity','Code','Year'])\n",
    "leftJoin.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leftJoin.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inner Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "innerJoin = pd.merge(population, taxes, how='inner', on=['Entity','Code','Year'])\n",
    "innerJoin.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "innerJoin.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "innerJoin[['Urban_Population','Tax_Percent_GDP']].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./diagrams/what.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Population is a string! Need to cast it to a numeric value.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "innerJoin.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "innerJoin['Population'] = pd.to_numeric(innerJoin['Urban_Population'], errors='coerce')\n",
    "innerJoin.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "innerJoin['Population'].hist()\n",
    "plt.title('Urban Population %')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "innerJoin[['Population','Tax_Percent_GDP']].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "innerJoin[['Population','Tax_Percent_GDP']].plot.scatter(x='Population', y='Tax_Percent_GDP', alpha=.1)\n",
    "plt.xlabel('% Urban Population')\n",
    "plt.ylabel('Tax (% GDP)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concatenating Data\n",
    "- Sometimes do to data collection convenience or file sizes, you may need to deal with a lot of files  \n",
    "- Sometimes the structure will be identical so you can effectively concatenate them together  \n",
    "- In order to loop through them you’ll likely need to use some of the base Python data structures (e.g., list, dictionary), loops, and/or the base libraries to make interacting with the file system more convenient  \n",
    "\n",
    "\n",
    "<img src='./diagrams/excel-pipeline.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example data: https://www.ssa.gov/OACT/babynames/limits.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install zipfile\n",
    "\n",
    "import requests\n",
    "from zipfile import ZipFile\n",
    "\n",
    "def download_file(file_name, url):\n",
    "    # Use requests Session with comprehensive browser headers\n",
    "    session = requests.Session()\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "        'Accept': '*/*',\n",
    "        'Accept-Language': 'en-US,en;q=0.9',\n",
    "        'Referer': 'https://www.ssa.gov/',\n",
    "        'Origin': 'https://www.ssa.gov',\n",
    "        'Connection': 'keep-alive',\n",
    "        'Sec-Fetch-Dest': 'document',\n",
    "        'Sec-Fetch-Mode': 'navigate',\n",
    "        'Sec-Fetch-Site': 'same-origin',\n",
    "    }\n",
    "    session.headers.update(headers)\n",
    "    \n",
    "    # First, try to get the main page to establish a session\n",
    "    try:\n",
    "        session.get('https://www.ssa.gov/OACT/babynames/state/', timeout=10)\n",
    "    except:\n",
    "        pass  # Continue even if this fails\n",
    "    \n",
    "    # Now download the file\n",
    "    response = session.get(url, headers=headers, timeout=30, allow_redirects=True)\n",
    "    response.raise_for_status()  # Raise an error for bad status codes\n",
    "    \n",
    "    # Ensure the directory exists\n",
    "    import os\n",
    "    os.makedirs(os.path.dirname(file_name), exist_ok=True)\n",
    "    \n",
    "    with open(file_name, 'wb') as file:\n",
    "        file.write(response.content)\n",
    "    print(f'Successfully downloaded {file_name}')\n",
    "        \n",
    "def unzip(file_name, path='./'):\n",
    "    with ZipFile(file_name, 'r') as zip: \n",
    "        zip.printdir() \n",
    "        print('Extracting all the files now...') \n",
    "        zip.extractall(path = path) \n",
    "        print('Done!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "\n",
    "data_url = 'https://www.ssa.gov/OACT/babynames/state/namesbystate.zip'\n",
    "filename='data/namesbystate.zip'\n",
    "fldr = 'data/namesbystate'\n",
    "\n",
    "if os.path.exists(fldr) == False:\n",
    "    print(\"data folder doesn't exist... creating & downloading data files\")\n",
    "    os.makedirs(fldr)\n",
    "    download_file(filename, data_url)\n",
    "    unzip(filename, fldr)\n",
    "\n",
    "fldrFiles = os.listdir(fldr)\n",
    "\n",
    "print(f'File count: {len(fldrFiles)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyze the types of files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fldrExt = {}\n",
    "for f in fldrFiles:\n",
    "    extention = f.split('.')[-1].lower()\n",
    "    fldrExt[extention] = fldrExt.get(extention, 0) + 1\n",
    "    \n",
    "fldrExt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loop through and import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filesImport = [x for x in fldrFiles if x.lower().endswith('.txt')]\n",
    "\n",
    "dfHolder = {}\n",
    "for f in filesImport:\n",
    "    dfHolder[f.split('.')[0]] = pd.read_csv(os.path.join(fldr, f),\n",
    "                                            header=None,\n",
    "                                            names=['state','gender','year','name','count'])\n",
    "    \n",
    "print(f'Number of files: {len(dfHolder)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Union the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(dfHolder, axis=0, ignore_index=True)\n",
    "\n",
    "dfx, dfy = df.shape\n",
    "print(f'Rows: {dfx:,}\\nColumns: {dfy:,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis Ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('year')['count'].sum().plot()\n",
    "plt.title('Number of Babies over time', loc='right')\n",
    "plt.xlabel('')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Most popular names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top 5 names for each gender over the past 5 years:\n",
    "(\n",
    "    df.query('year >= 2016')\n",
    "    .groupby(['gender','name'])['count']\n",
    "    .sum()\n",
    "    .groupby('gender')\n",
    "    .nlargest(5)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the influence of celebrities on naming trends\n",
    "Can we associate naming trends with celebrity popularity?\n",
    "<img src='./diagrams/elvis.jpg'>\n",
    "Image Source: Wikipedia\n",
    "\n",
    "#### Albums:\n",
    "- Studio albums  \n",
    "- Elvis Presley (1956)  \n",
    "- Elvis (1956)  \n",
    "- Elvis' Christmas Album (1957)  \n",
    "- Elvis is Back! (1960)  \n",
    "- His Hand in Mine (1960)  \n",
    "- Something for Everybody (1961)  \n",
    "- Pot Luck (1962)  \n",
    "- Elvis for Everyone! (1965)  \n",
    "- How Great Thou Art (1967)  \n",
    "- From Elvis in Memphis (1969)  \n",
    "- From Memphis to Vegas / From Vegas to Memphis (1969)  \n",
    "- That's the Way It Is (1970)  \n",
    "- Elvis Country (I'm 10,000 Years Old) (1971)  \n",
    "- Love Letters from Elvis (1971)  \n",
    "- Elvis sings The Wonderful World of Christmas (1971)  \n",
    "- Elvis Now (1972)  \n",
    "- He Touched Me (1972)  \n",
    "- Elvis (1973) (The \"Fool\" Album)  \n",
    "- Raised on Rock / For Ol' Times Sake (1973)  \n",
    "- Good Times (1974)  \n",
    "- Promised Land (1975)  \n",
    "- Today (1975)  \n",
    "- From Elvis Presley Boulevard, Memphis, Tennessee (1976)  \n",
    "- Moody Blue (1977)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_name_popularity_over_years(name):\n",
    "    print(sum(df.name == name))\n",
    "    df.query('name==@name').groupby('year')['count'].sum().plot()\n",
    "    plt.title(f'Trend of Babies Named \"{name}\"', loc='center')\n",
    "    plt.xlabel('')\n",
    "    plt.ylabel('')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(df.name == 'Elvis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_name_popularity_over_years('Elvis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_name_popularity_over_years('Ariana')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_name_popularity_over_years('Elon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_name_popularity_over_years('Alexa')\n",
    "# why Alexa name dropped ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check_name_popularity_over_years('Taylor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources\n",
    "[Edward Tufte](https://www.edwardtufte.com/tufte/)\n",
    "<br>[Stephen Few](https://www.perceptualedge.com)\n",
    "<br>[Save The Pies for Dessert](https://www.perceptualedge.com/articles/visual_business_intelligence/save_the_pies_for_dessert.pdf)\n",
    "<br>[Tableau Public Gallery](https://public.tableau.com/en-us/gallery/?tab=viz-of-the-day&type=viz-of-the-day)\n",
    "<br>[Matplotlib](https://matplotlib.org)\n",
    "<br>[seaborn](https://seaborn.pydata.org)\n",
    "<br>[JunkCharts - examples of what not to do](https://junkcharts.typepad.com)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some refreshers\n",
    "\n",
    "https://github.com/msaricaumbc/DS601/blob/main/Week03/week03_2.ipynb\n",
    "\n",
    "https://github.com/msaricaumbc/DS601/blob/main/Week04/week04%20-%20data%20transpormation%202.ipynb\n",
    "\n",
    "https://github.com/msaricaumbc/DS601/blob/main/Week07/week7%20relational%20data.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
